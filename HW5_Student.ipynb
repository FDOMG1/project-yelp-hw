{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl0OH3XGTXe1"
   },
   "source": [
    "# HW5 Skeleton Code\n",
    "Please note that this skeleton code is provided to help you with homework.\n",
    "Full description of each question can be found on HW5.pdf, so please read instruction of each question carefully. There might be some questions that is not presented in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1QYqL7lvuvmK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awc_PHjUuvmR"
   },
   "source": [
    "## Q. Changing HTML Text to Plain Text\n",
    "\n",
    "The Python library <b>BeautifulSoup</b> is useful for dealing with html text. In order to use this library, you will need to install it first by running the following command:\n",
    " <b>conda install beautifulsoup4</b>\n",
    " in the terminal.\n",
    " <br> In the code, you can import it by running the following line:\n",
    "<br>\n",
    "  <b>from bs4 import BeautifulSoup </b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JaHFZFOuuvmU"
   },
   "outputs": [],
   "source": [
    "#Read our data file\n",
    "\n",
    "df_train = pd.read_csv('stack_stats_2024_train.csv') #Todo\n",
    "df_test = pd.read_csv('stack_stats_2024_test.csv') #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jzyC4JZpuvmW"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3530368254.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\12985\\AppData\\Local\\Temp\\ipykernel_31696\\3530368254.py\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    df_train['Tags'] =\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Cleaning 'Body'\n",
    "#Change HTML Text to Plain text using get_text() function from BeautifulSoup\n",
    "#If you are not familiar with the apply method, please check discussion week 10 lecture and code.\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(str.lower ) #Todo\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(document):\n",
    "\n",
    "    no_punct = ''.join([character for character in document if character not in punctuation])\n",
    "    \n",
    "    return no_punct\n",
    "def remove_digit(document): \n",
    "    \n",
    "    no_digit = ''.join([character for character in document if not character.isdigit()])\n",
    "              \n",
    "    return no_digit\n",
    "#Manually cleaned up newline tag \\n and tab tag \\t.\n",
    "df_train['Body'] = df_train['Body'].apply(remove_punctuation ) #Todo\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_digit ) #Todo\n",
    "#If you need any other cleaning process, please uncomment the below.\n",
    "#df_train['Body'] = df_train['Body'].apply(lambda ) #Todo\n",
    "\n",
    "#Cleaning Tags\n",
    "#This would be somewhat similar to the above.\n",
    "\n",
    "#Todo: Clean Tags, please feel free to add any lines below\n",
    "df_train['Tags'] =\n",
    "\n",
    "\n",
    "#Todo: Repeat the same process for test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNS-3DkCuvmb"
   },
   "source": [
    "## Q. Basic Text Cleaning and Merging into a single Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "031sBi5Duvmc"
   },
   "source": [
    "### Change to Lower Case, Remove puncuation, digits,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-YUYKlvuvmd"
   },
   "outputs": [],
   "source": [
    "#Change to Lowercase\n",
    "\n",
    "df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].applymap() #Todo, do you see why we used applymap instead of apply in this case?\n",
    "df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].applymap() #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bbcp_w_wuvme"
   },
   "outputs": [],
   "source": [
    "#Remove Punctations\n",
    "from string import punctuation\n",
    "\n",
    "#You can get this function from our discussion session code. However, we leave it as a blank for a practice.\n",
    "def remove_punctuation(document):\n",
    "\n",
    "    no_punct = #Todo\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "df_train[['Body','Title','Tags']] = #Todo\n",
    "df_test[['Body','Title','Tags']] = #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNUiymSJuvmh"
   },
   "outputs": [],
   "source": [
    "#Remove Digits\n",
    "\n",
    "def remove_digit(document):\n",
    "\n",
    "    no_digit = #Todo\n",
    "\n",
    "    return no_digit\n",
    "\n",
    "df_train[['Body','Title','Tags']] = #Todo\n",
    "df_test[['Body','Title','Tags']] = #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAsCQQPQuvmi"
   },
   "source": [
    "### Tokenization and Remove Stopwords and do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25821,
     "status": "ok",
     "timestamp": 1668123057199,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "gOrVZxXQuvmj",
    "outputId": "07dd90e0-e5ca-4634-f43f-f379fe3a0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "df_train[['Body','Title','Tags']] = #Todo\n",
    "df_test[['Body','Title','Tags']] = #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2684,
     "status": "ok",
     "timestamp": 1668123197885,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "82DJNdV1uvmj",
    "outputId": "8f8148ae-7551-43fc-d3d3-2bc8d8202d15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(document):\n",
    "\n",
    "    words = #Todo\n",
    "\n",
    "    return words\n",
    "\n",
    "df_train[['Body','Title','Tags']] = #Todo\n",
    "df_test[['Body','Title','Tags']] = #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54S-irnRuvmk"
   },
   "outputs": [],
   "source": [
    "#We use porter stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemmer(document):\n",
    "\n",
    "    stemmed_document = #Todo\n",
    "\n",
    "    return stemmed_document\n",
    "\n",
    "df_train[['Body','Title','Tags']] = #Todo\n",
    "df_test[['Body','Title','Tags']] = #Todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2sfBcaMLTok"
   },
   "source": [
    "## Let's Check our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6V33S7pLS76"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFgV2VL0uvml"
   },
   "source": [
    "### Q. Treat Three text data independently and merge into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCB-voc_uvmm"
   },
   "outputs": [],
   "source": [
    "#Treat Three types of data independently\n",
    "#let's define functions that will help this operation\n",
    "\n",
    "def add_body(document):\n",
    "\n",
    "    added_document = #Todo\n",
    "\n",
    "    return added_document\n",
    "\n",
    "def add_title(document):\n",
    "\n",
    "    added_document = #Todo\n",
    "\n",
    "    return added_document\n",
    "\n",
    "def add_tags(document):\n",
    "\n",
    "    added_document = #Todo\n",
    "\n",
    "    return added_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWy0BGVHuvmm"
   },
   "outputs": [],
   "source": [
    "df_train['Body'] = df_train['Body'].apply(add_body)\n",
    "df_train['Title'] = df_train['Title'].apply(add_title)\n",
    "df_train['Tags'] = df_train['Tags'].apply(add_tags)\n",
    "\n",
    "df_test['Body'] = df_test['Body'].apply(add_body)\n",
    "df_test['Title'] = df_test['Title'].apply(add_title)\n",
    "df_test['Tags'] = df_test['Tags'].apply(add_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgcE1Vcnuvmn"
   },
   "outputs": [],
   "source": [
    "#Now we need to merge all those 3 columns into a single column. Implement this below.\n",
    "df_train['text'] = #Todo\n",
    "df_test['text'] = #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq2PYhnmLhDi"
   },
   "source": [
    "## Let's check our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtYP_Lu3LjIb"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4esXb6WGuvmo"
   },
   "source": [
    "### Q. Detokenize and convert to document term matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwhJ1pVBuvmo"
   },
   "outputs": [],
   "source": [
    "#Merge Three text column into one column and detokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_train = df_train['text'].apply() #Todo: Detokenize your tokenized text data\n",
    "countvec_train = #Todo: Define your own CountVectorizer here\n",
    "sparse_dtm_train = #Todo: Fit and Transform your Countvectorizer and return sparse dtm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoO-e-00uvmo"
   },
   "outputs": [],
   "source": [
    "#Todo: Do same on the test set.\n",
    "text_test = df_test['text'].apply()\n",
    "sparse_dtm_test ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1668128857596,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "hiK01v4luvmp",
    "outputId": "f6521cdc-fd1a-446f-9a7c-eb03df04a5f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Convert the sprase dtm to pandas DataFrame.\n",
    "dtm_train = #Todo\n",
    "dtm_test = #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVDS6druvmq"
   },
   "source": [
    "### Q. Change dependent variable to binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSQu_SPquvmq"
   },
   "outputs": [],
   "source": [
    "#Change 'Score' to a binary variable, which indicates whether the question is good or not.\n",
    "y_train = #Todo\n",
    "y_test = #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWhfmKlYuvmr"
   },
   "outputs": [],
   "source": [
    "#Add y_train and y_test to your data frame if it is needed. Drop unnecessary columns\n",
    "df_train[''] = y_train\n",
    "df_test[''] = y_test\n",
    "df_train.drop(columns = [], inplace = True)\n",
    "df_test.drop(columns = [], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiYas_A6JcS1"
   },
   "source": [
    "## Let's check our DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeTgM0ZMJqbI"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVPT48NRuvms"
   },
   "source": [
    "## (b) Please read the instruction carefully in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yD6xH8PzyoZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O3EsbQGU0uX"
   },
   "outputs": [],
   "source": [
    "#Create Comparison Table\n",
    "#These lines are provided for you to help construct a comparison table.\n",
    "#It is not requred to follow this format. + You need to find ACC, TPR, FPR, PRE for each model that you choose.\n",
    "comparison_data = {'Baseline':[baseline_acc,baseline_TPR,baseline_FPR, baseline_PRE],\n",
    "                   'Logistic Regression':[log_acc,log_TPR,log_FPR, log_PRE],\n",
    "                   'Decision Tree Classifier':[dtc_acc,dtc_TPR,dtc_FPR,dtc_PRE],\n",
    "                   'Random Forest with CV':[rf_acc,rf_TPR, rf_FPR,rf_PRE],\n",
    "                  'Linear Discriminant Analysis':[lda_acc,lda_TPR, lda_FPR,lda_PRE]}\n",
    "\n",
    "comparison_table = pd.DataFrame(data=comparison_data, index=['Accuracy', 'TPR', 'FPR','PRE']).transpose()\n",
    "comparison_table.style.set_properties(**{'font-size': '12pt',}).set_table_styles([{'selector': 'th', 'props': [('font-size', '10pt')]}])\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjsc7lf6zOms"
   },
   "source": [
    "\n",
    "## Report details of your training procedures and final comparisons on the test set in this cell. Use your best judgment to choose a final model and explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLa-KDajuvmz"
   },
   "source": [
    "## Report Bootstrap Analysis in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrCM_96Quvm2"
   },
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh2mvNrrTFBU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1AHr2NIU61Oco-PfG10o3gAYXLrsyJ8Sv",
     "timestamp": 1668122684758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
